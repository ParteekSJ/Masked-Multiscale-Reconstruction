import syssys.path.append("../")import torchfrom torch import nnfrom torch.optim import Adamimport matplotlib.pyplot as pltfrom torch.optim.lr_scheduler import LinearLR, MultiStepLR, ChainedSchedulerfrom config.aebad_V_config import get_cfg # ORIGINAL LEARNING RATE SCHEDULERdef mmr_adjust_learning_rate(optimizer, epoch, cfg):    """cosine lr"""    if epoch < cfg.TRAIN_SETUPS.warmup_epochs:        lr = cfg.TRAIN_SETUPS.learning_rate * epoch / cfg.TRAIN_SETUPS.warmup_epochs    else:        if epoch + 1 == 120:            cfg.TRAIN_SETUPS.learning_rate /= 10        elif epoch + 1 == 160:            cfg.TRAIN_SETUPS.learning_rate /= 10        lr = cfg.TRAIN_SETUPS.learning_rate    # lr *= 0.5 * (1. + math.cos(math.pi * epoch / cfg.TRAIN_SETUPS.epochs))    for param_group in optimizer.param_groups:        param_group["lr"] = lr    return lr# DOESN'T WORK.def custom_mmr_adjust_learning_rate(optimizer, cfg):    warmup_scheduler = LinearLR(        optimizer,        start_factor=cfg.TRAIN_SETUPS.learning_rate,        end_factor=1.0,        total_iters=cfg.TRAIN_SETUPS.warmup_epochs,    )        step_scheduler = MultiStepLR(optimizer, milestones=[120, 160], gamma=0.1)    scheduler = ChainedScheduler([warmup_scheduler, step_scheduler])    return schedulerif __name__ == "__main__":    model = nn.Linear(10, 10)    aebad_v_cfg = get_cfg()    warmup_epochs = 50    optimizer1 = Adam(        params=model.parameters(), lr=0.001, weight_decay=0.05, betas=(0.9, 0.95)    )    optimizer2 = Adam(        params=model.parameters(), lr=0.001, weight_decay=0.05, betas=(0.9, 0.95)    )    scheduler2 = custom_mmr_adjust_learning_rate(optimizer2, aebad_v_cfg)    for epoch in range(200):        optimizer1.step()        current_lr = mmr_adjust_learning_rate(optimizer1, epoch, aebad_v_cfg)        optimizer2.step()        scheduler2.step()                if (epoch + 1) % 50 == 0:            print(f"{optimizer1.param_groups[0]['lr']:.5f}")            print(f"{optimizer2.param_groups[0]['lr']:.5f}")'''0.000980.0010.00011e-05'''