import syssys.path.append("../")import pprintfrom collections import Counterimport torchimport matplotlib.pyplot as pltfrom torch.optim.lr_scheduler import LinearLR, MultiStepLR, ChainedSchedulerfrom config.default import get_cfg# Custom Learning Rate Schedulerdef mmr_adjust_lr(optimizer, epoch, cfg):    # Warm up phase    if epoch < cfg.TRAIN_SETUPS.warmup_epochs:        # learning rate linearly increases from 0 to the initial learning rate        lr = cfg.TRAIN_SETUPS.learning_rate * epoch / cfg.TRAIN_SETUPS.warmup_epochs    # Main training phase    # After warm up phase, a STEP-DECAY schedule is used.    else:        if epoch + 1 == 120:            cfg.TRAIN_SETUPS.learning_rate /= 10        elif epoch + 1 == 160:            cfg.TRAIN_SETUPS.learning_rate /= 10        lr = cfg.TRAIN_SETUPS.learning_rate    # Apply the new learning rate    for param_group in optimizer.param_groups:        param_group["lr"] = lr    return lr# Custom Learning Rate Scheduler using PyTorch's built-in methodsdef mmr_lr_custom_scheduler(optimizer, cfg):    warmup_scheduler = LinearLR(        optimizer,        start_factor=1e-8,        end_factor=1.0,        total_iters=cfg.TRAIN_SETUPS.warmup_epochs,    )    step_scheduler = MultiStepLR(optimizer, milestones=[120, 160], gamma=0.1)    scheduler = ChainedScheduler([warmup_scheduler, step_scheduler])    return schedulerdef pretty_print_scheduler_state(state_dict):    formatted = {"last_lr": state_dict["_last_lr"], "schedulers": []}    for scheduler in state_dict["_schedulers"]:        scheduler_info = {            "type": scheduler.__class__.__name__,            "base_lr": scheduler["base_lrs"][0],            "last_epoch": scheduler["last_epoch"],            "last_lr": scheduler["_last_lr"][0],        }        if "start_factor" in scheduler:  # LinearLR            scheduler_info.update(                {                    "start_factor": scheduler["start_factor"],                    "end_factor": scheduler["end_factor"],                    "total_iters": scheduler["total_iters"],                }            )        elif "milestones" in scheduler:  # MultiStepLR            scheduler_info.update(                {                    "milestones": list(scheduler["milestones"].keys()),                    "gamma": scheduler["gamma"],                }            )        formatted["schedulers"].append(scheduler_info)    pprint.pprint(formatted, width=1, sort_dicts=False)if __name__ == "__main__":    cfg = get_cfg()    # Create a dummy model and optimizer    model = torch.nn.Linear(10, 1)    optimizer1 = torch.optim.SGD(model.parameters(), lr=cfg.TRAIN_SETUPS.learning_rate)    optimizer2 = torch.optim.SGD(model.parameters(), lr=cfg.TRAIN_SETUPS.learning_rate)    # Create scheduler for the second method    scheduler = mmr_lr_custom_scheduler(optimizer2, cfg)    pretty_print_scheduler_state(scheduler.state_dict())    """        {'last_lr': [5e-11],     'schedulers': [{'type': 'dict',                     'base_lr': 0.005,                     'last_epoch': 0,                     'last_lr': 5e-11,                     'start_factor': 1e-08,                     'end_factor': 1.0,                     'total_iters': 40},                    {'type': 'dict',                     'base_lr': 0.005,                     'last_epoch': 0,                     'last_lr': 5e-11,                     'milestones': [120,                                    160],                     'gamma': 0.1}]}        """    # Simulate 200 epochs and record learning rates    lr_custom = []    lr_pytorch = []    for epoch in range(200):        lr_custom.append(mmr_adjust_lr(optimizer1, epoch, cfg))        scheduler.step()        lr_pytorch.append(scheduler.get_last_lr()[0])    # Plot the results    plt.figure(figsize=(12, 6))    plt.plot(lr_custom, label="Custom Function")    plt.plot(lr_pytorch, label="PyTorch Scheduler")    plt.xlabel("Epoch")    plt.ylabel("Learning Rate")    plt.title("Comparison of Learning Rate Schedules")    plt.legend()    plt.yscale("log")    plt.grid(True)    plt.show()    # Print the final learning rates    print(f"Final LR (Custom): {lr_custom[-1]}")    print(f"Final LR (PyTorch): {lr_pytorch[-1]}")